# Evaluation: Planning and Usability Studies

## Why evaluate?
- to gather information about the usability and user experience of prototypes, applications and other interfaces
  - ie. does the interface meet the required usability and UX goals?
- verify design is appropriate
- reduce debate between designers as to which designs actually work
- test hypotheses
- better products sell better

## Why evaluate... with users?
- designers know too much about their product to be objective judges
- designers are not users!

## What do we evaluate?
- depends on your goals and stage
  - user requirements
  - UX requirements
  - aesthetics
  - "which menu type works best?"
  - etc.

## Where to evaluate?
- laboratory: high control data capture
- users' environments: less control over variables, but better representation of the 'real world'
- remotely: can have lots of participants
- anywhere: you can test anyways, just be aware of the variables that are being affected

## Types of evaluation
- controlled settings with users: usability testing and experiments
- natural settings involving users: field studies
- any settings without users: inspections

### Controlled settings involving users
- usability testing: determine whether an interface is usable by intended user population by asking them to complete tasks
  - usually done in a lab or other controlled setting
  - pro: good at finding large usability issues
  - cons: lacks context and is intrusive

### Natural settings involving users
- field studies: evaluate in natural settings
  - used more often in requirements than evaluation, but can be used to understand how new technology affects users
- pros: observes users under normal context
- cons: expensive and intrusive: difficult to replicate

### Settings not involving users
- inspections: cognitive walkthroughs and heuristic evaluations; experts get in the mindset of users
- pros: quick and cheap
- designers are still not users, so may miss subtle aspects of user experience

## Evaluation process:
- usability and UX requirements
- form an evaluation strategy
- create an evaluation plan
- conduct evalutaion sessions
- analyze data
- interpret results
- are the requirements met?
  - if yes, you are done!
  - if not, update the UI design

## Usability and UX Requirements refresher

Usability
  - effectiveness
  - efficiency
  - safety
  - utility
  - learnability
  - memorability

UX
  - how people feel about a product
  - goals depend on your users, the domain and the application

## Evaluation strategies
- describes what you want to achieve by conducting the evaluation sessions
- lists contraints (time, money, access to users or resources, etc.)

## Questions to answer in your evaluation strategy
- what is the purpose of this evaluation? what do you hope to learn?
  - does the application meet usability and UX requirements?
  - are there other concerns we need to test?
- what type of data are we collecting?
  - qualitative and/or quantitative?
- what am I evaluating?
  - high fidelity, low fidelity, requirements, use cases, etc.
- what constraints are there?
  - time, money, resuorces, users we have access to

## Create an evaluation plan

### Who, what where, when, why and how
Why
- what questions are you trying to answer? these should come from usability and UX goals

Who
- how many users?
- how will you recruit and get consent?
- should be based on personas

What
- which tasks are  you going to evaluate? these should come from use cases.
- what data are you going to obtain? these should be related to the questions you are attempting to answer

How
- what type of evaluation or study?
  - usability study, experiment, or inspection?
- how will you interpret the data?

Where
- where will you perform the study and why?

## Usability studies
First rule of usability: don't listen to users.
- to design the best UX, pay attention to what the users do, not what they say they do
  - self-reported claims are unreliable, as are user speculations about future behavior

### Usability study session
1. welcome and introduction
2. advise participant of any necessary info about the prototype
3. observe participants as they perform tasks
4. follow up interview (dbriefing) or questionnaire
5. final thanks

### Preparing for a usability study

1. Define team member roles
2. define task descriptions
3. create evaluation script
4. pilot study
5. perform study

#### Determine evaluation roles
- facilitator
- note taker
- equipment operator
- observer
- meeter and greeter
- recruiter
- the lone evaluator

#### Prepare ask descriptions
- generate task descriptions for each use case
- create task cards, one for each task
  - simply a card with the task description on it that you can show the participant

Here's an example task description:<br/>
Assume you are planning a road trip that will take about 2 hours each way and you'd like to create a playlist on Spotify's desktop app that you can access on your phone while driving. You'd like a mix of songs from you current library and new discoveries. Please create a playlist that is long enough to occupy your time while driving.

#### Create evaluation scrip
- it will be used to guide the facilitator
- ensures that all participants receive the same information, especially with different facilitators
- how detailed a script you use depends on how important it is to control the interactions
- helps you as an evaluator to remember all important information

- cons: may be difficult to stick to
- may become difficult to maintain with rapid prototyping

Your evaluation script should contain:
1. welcome
2. briefing about evaluation
3. brief explanation of consent form
4. introduction to the tasks
5. brief description of each task
6. post evaluation interview script and/or questionnaire
7. final thanks

#### Pilot study
- ensures session will work as expected
- process of debugging or testing evaluation material
- determine how long the study will take
- get practice

#### Perform study
Present participants with one tasks at a time and observe them
  - think aloud technique: ask user to think aloud as they step through a task 
  - don't interfere or try to correct participant (especially if time is being measured)
- watch with an impartial eye
- don't take criticism personally

### Collecting data
What to collect
- notes about interactions
- quantitative data: watching and measuring what participants do

### Quantitative performance measures
- time to complete a task
- number and type of errors per task
- number of users making a particular error
- number of users completing a task successfully
- number of steps to completion
- number of times user corrects / repeats an action
- keystroke logging

### Post session discussion
- debriefing
  - allow users to reflect on their interaction with the prototype
- gain knowledge on the user experience from their perspective
- questionnaires: help to quantify usability and the users' experiences


